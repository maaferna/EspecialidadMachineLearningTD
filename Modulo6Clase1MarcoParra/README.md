# ğŸ“Š Proyecto de Clustering JerÃ¡rquico con PCA y t-SNE

## ğŸ“Œ IntroducciÃ³n

Este proyecto tiene como objetivo aplicar **tÃ©cnicas de clustering jerÃ¡rquico aglomerativo** en datasets multivariados (Iris y Wine) para identificar estructuras ocultas en los datos.
Se utilizarÃ¡n mÃ©todos de reducciÃ³n de dimensionalidad (**PCA** y **t-SNE**) con el fin de visualizar mejor los patrones y evaluar la calidad de los agrupamientos generados.

La idea central es **explorar la similitud entre observaciones sin necesidad de etiquetas supervisadas** y posteriormente analizar los resultados mediante grÃ¡ficos y mÃ©tricas descriptivas.

---

## ğŸ¯ Objetivos del proyecto

1. **Preprocesar los datos** (normalizaciÃ³n, eliminaciÃ³n de outliers, revisiÃ³n de varianza).
2. **Aplicar Clustering JerÃ¡rquico Aglomerativo** con diferentes configuraciones:

   * MÃ©todo **Ward** (minimiza la varianza intra-cluster).
   * MÃ©todo **Average** (utiliza la distancia promedio entre elementos).
3. **Generar dendrogramas** para visualizar la estructura jerÃ¡rquica de los clusters.
4. **Comparar resultados con PCA y t-SNE** en 2D y 3D.
5. **Analizar las diferencias** entre la proyecciÃ³n PCA y t-SNE en la identificaciÃ³n de grupos.
6. Documentar los resultados en grÃ¡ficos y tablas exportadas a la carpeta `outputs/`.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

* **Lenguaje principal:** Python 3.8
* **LibrerÃ­as de Machine Learning y anÃ¡lisis**:

  * `scikit-learn` â†’ clustering, PCA, t-SNE
  * `scipy` â†’ linkage y dendrogramas
  * `numpy` y `pandas` â†’ manipulaciÃ³n de datos
* **VisualizaciÃ³n**:

  * `matplotlib` â†’ grÃ¡ficos 2D y 3D
  * `seaborn` â†’ visualizaciones estadÃ­sticas
* **Entorno**: Conda (`environment.yml` con dependencias)
* **Otros**: `openml` para cargar datasets pÃºblicos

---

## ğŸ“‚ Estructura de carpetas

```
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ main.py              # Archivo principal del pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils.py             # MÃ©todos para carga y preprocesamiento de datos
â”‚   â”œâ”€â”€ modelos.py           # DefiniciÃ³n de modelos de clustering
â”‚   â”œâ”€â”€ evaluador.py         # Funciones para evaluar PCA y clustering
â”‚   â””â”€â”€ visualizador.py      # GrÃ¡ficas (PCA, t-SNE, dendrogramas)
â”œâ”€â”€ outputs/                 # Resultados exportados (grÃ¡ficas, CSVs)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analisis.ipynb       # Notebook con resultados y visualizaciones
â”œâ”€â”€ environment.yml          # Dependencias Conda del proyecto
â””â”€â”€ README.md                # DocumentaciÃ³n del proyecto
```

---

## âš™ï¸ Flujo de trabajo del proyecto

1. **Carga de dataset** desde `utils.py`

   * Dataset elegido: Iris o Wine desde `sklearn.datasets`.
   * Escalado con `StandardScaler` o `MinMaxScaler`.

2. **ReducciÃ³n de dimensionalidad (PCA y t-SNE)**

   * PCA â†’ cÃ¡lculo de varianza acumulada y proyecciones.
   * t-SNE â†’ proyecciÃ³n no lineal para comparaciÃ³n.

3. **Clustering jerÃ¡rquico aglomerativo**

   * ConfiguraciÃ³n con mÃ©todos **Ward** y **Average**.
   * EvaluaciÃ³n de 2 y 3 clusters.
   * VisualizaciÃ³n con dendrogramas y proyecciones.

4. **ExportaciÃ³n de resultados**

   * CSV con mÃ©tricas de PCA.
   * GrÃ¡ficas de PCA y t-SNE en 2D y 3D.
   * Dendrogramas comparativos.

---


# ğŸ“Š Informe de Resultados â€“ Clustering JerÃ¡rquico con PCA y t-SNE

## ğŸ“ IntroducciÃ³n

El objetivo de este proyecto fue aplicar **clustering jerÃ¡rquico aglomerativo** sobre datasets multivariados (ejemplo: *Iris*), utilizando mÃ©todos de enlace **Ward** y **Average**, y posteriormente visualizar y analizar la estructura de los grupos formados.

Se emplearon dos tÃ©cnicas de reducciÃ³n de dimensionalidad para facilitar la interpretaciÃ³n:

1. **PCA (AnÃ¡lisis de Componentes Principales)** â†’ Busca proyectar los datos en menos dimensiones **maximizando la varianza explicada**.
2. **t-SNE (t-distributed Stochastic Neighbor Embedding)** â†’ Busca proyectar los datos en un espacio de baja dimensiÃ³n **preservando la estructura local** (distancias entre puntos vecinos).

AdemÃ¡s, se generaron **dendrogramas** para visualizar la jerarquÃ­a de los clusters formados.

---

## ğŸ“ AnÃ¡lisis de GrÃ¡ficos

### ğŸ”¹ 1. PCA en 2D

![PCA 2D](outputs/pca_2d.png)

En este grÃ¡fico, los datos fueron reducidos a **2 componentes principales**.

**InterpretaciÃ³n:**

* Los colores representan las clases reales o clusters detectados.
* Se observa que los puntos se agrupan formando **tres regiones principales**:

  * El grupo de la izquierda se encuentra bien separado.
  * Los dos grupos de la derecha tienen una ligera superposiciÃ³n, lo que indica que PCA conserva bastante informaciÃ³n, pero no logra una separaciÃ³n perfecta.
* El **eje X (Componente Principal 1)** captura la mayor varianza, seguido del **eje Y (Componente Principal 2)**.

ğŸ“Œ **ConclusiÃ³n parcial:** PCA 2D permite una visualizaciÃ³n clara de la estructura general, especialmente destacando un cluster muy definido.

---

### ğŸ”¹ 2. PCA en 3D

![PCA 3D](outputs/pca_3d.png)

AquÃ­ los datos se proyectaron en **3 componentes principales**.

**InterpretaciÃ³n:**

* La tercera dimensiÃ³n ayuda a **mejorar la separaciÃ³n** de los clusters que en 2D se solapaban parcialmente.
* Se distinguen **tres agrupamientos claros**, reforzando que la estructura intrÃ­nseca de los datos efectivamente corresponde a 3 grupos.
* El PCA sigue siendo lineal, por lo que no capta relaciones altamente no lineales, pero es suficiente para este dataset.

ğŸ“Œ **ConclusiÃ³n parcial:** PCA 3D confirma la evidencia visual de tres clusters claros, mejorando la discriminaciÃ³n respecto al PCA 2D.

---

### ğŸ”¹ 3. Dendrograma â€“ Average (3 clusters)

![Dendrograma Average 3](outputs/dendrograma_average_3.png)

Un **dendrograma** muestra la jerarquÃ­a de fusiones de los clusters.

**InterpretaciÃ³n:**

* El eje Y indica la **distancia de enlace** (quÃ© tan similares son los grupos antes de fusionarse).
* Con el mÃ©todo **Average**, cada fusiÃ³n se calcula en base al **promedio de distancias entre todos los puntos de dos clusters**.
* Se observa un **salto grande en la altura** cuando se pasa de 3 a 2 clusters, lo que sugiere que **3 clusters es una particiÃ³n natural**.

ğŸ“Œ **ConclusiÃ³n parcial:** Este dendrograma respalda la elecciÃ³n de 3 clusters, ya que cortar el Ã¡rbol a esa altura preserva la estructura natural.

---

### ğŸ”¹ 4. t-SNE â€“ Average (3 clusters)

![t-SNE Average 3](outputs/tsne_average_3.png)

**InterpretaciÃ³n:**

* A diferencia de PCA, t-SNE no busca maximizar varianza, sino preservar **distancias locales**.
* Los tres clusters aparecen **muy bien definidos** y separados, con **menor solapamiento** que en PCA 2D.
* Esto indica que t-SNE puede capturar **estructuras no lineales** que PCA no logra visualizar.

ğŸ“Œ **ConclusiÃ³n parcial:** t-SNE es mÃ¡s efectivo que PCA 2D para mostrar separaciones claras entre clusters.

---

### ğŸ”¹ 5. ComparaciÃ³n de 2 y 3 Clusters (Ward y Average)

* **Average con 2 clusters:**
  ![t-SNE Average 2](outputs/tsne_average_2.png)
  ![Dendrograma Average 2](outputs/dendrograma_average_2.png)
  â Los datos se dividen en dos grandes grupos, pero la separaciÃ³n pierde detalle respecto a 3 clusters.

* **Ward con 3 clusters:**
  ![t-SNE Ward 3](outputs/tsne_ward_3.png)
  ![Dendrograma Ward 3](outputs/dendrograma_ward_3.png)
  â El mÃ©todo **Ward** minimiza la varianza interna de cada cluster, lo que genera **clusters mÃ¡s compactos y equilibrados**.

* **Ward con 2 clusters:**
  ![t-SNE Ward 2](outputs/tsne_ward_2.png)
  ![Dendrograma Ward 2](outputs/dendrograma_ward_2.png)
  â Similar a Average 2, pero con una estructura mÃ¡s balanceada en tamaÃ±o.

ğŸ“Œ **ConclusiÃ³n comparativa:**

* **Ward** tiende a producir clusters mÃ¡s equilibrados y compactos.
* **Average** refleja mÃ¡s la densidad relativa entre grupos.
* Con **2 clusters**, se simplifica demasiado la estructura.
* Con **3 clusters**, se obtiene la mejor representaciÃ³n de la variabilidad de los datos.

---

## ğŸ“Œ Diferencias clave entre PCA y t-SNE

| Aspecto                | PCA                                 | t-SNE                                   |
| ---------------------- | ----------------------------------- | --------------------------------------- |
| Tipo de reducciÃ³n      | Lineal (proyecciones ortogonales)   | No lineal (preserva distancias locales) |
| Interpretabilidad      | Alta (basada en varianza explicada) | Media (parÃ¡metros sensibles, no lineal) |
| SeparaciÃ³n de clusters | Buena, pero puede solaparse         | Excelente para visualizar separaciones  |
| Criterio de selecciÃ³n  | Varianza explicada acumulada        | DistribuciÃ³n visual de agrupamientos    |

---

## ğŸ“Š Resumen Ejecutivo

* **El PCA 2D y 3D confirmaron que la estructura natural del dataset es de 3 clusters**, aunque el PCA 2D mostrÃ³ cierto solapamiento que se redujo en PCA 3D.
* **t-SNE demostrÃ³ una separaciÃ³n mÃ¡s clara y definida**, mostrando la capacidad de este mÃ©todo para capturar relaciones no lineales.
* **Los dendrogramas evidenciaron un salto en la distancia de enlace al pasar de 3 a 2 clusters**, validando que 3 es la mejor particiÃ³n.
* El mÃ©todo **Ward produjo clusters mÃ¡s compactos**, mientras que **Average reflejÃ³ mejor las densidades relativas**.
* **ConclusiÃ³n final:** Para este dataset, la combinaciÃ³n de PCA (como exploraciÃ³n inicial) y t-SNE (para separaciÃ³n mÃ¡s clara) con **3 clusters** ofrece la representaciÃ³n mÃ¡s coherente de la estructura de los datos.

---



## ğŸ“‘ Resumen Ejecutivo

El presente proyecto implementa un pipeline para analizar y visualizar **estructuras jerÃ¡rquicas de clusters** en datasets multivariados.
A travÃ©s del uso de **PCA y t-SNE**, se explora cÃ³mo distintas tÃ©cnicas de reducciÃ³n de dimensionalidad impactan la representaciÃ³n visual de los datos y la claridad de los agrupamientos.

Los principales hallazgos esperados son:

* IdentificaciÃ³n de **agrupamientos naturales** en Iris y Wine.
* ConfirmaciÃ³n de que **Ward** suele producir clusters mÃ¡s compactos, mientras que **Average** puede resaltar relaciones mÃ¡s difusas.
* DemostraciÃ³n de que PCA y t-SNE ofrecen **perspectivas complementarias**:

  * PCA conserva la varianza global.
  * t-SNE enfatiza la **estructura local** de los datos.

La comparaciÃ³n de ambos mÃ©todos permitirÃ¡ **evaluar fortalezas y limitaciones** en la detecciÃ³n de patrones y justificar cuÃ¡l serÃ­a mÃ¡s recomendable para problemas de clasificaciÃ³n sin supervisiÃ³n en entornos reales.

---


