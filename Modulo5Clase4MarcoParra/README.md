
# Proyecto: AplicaciÃ³n de TÃ©cnicas de ValidaciÃ³n Cruzada

## ğŸ¯ Objetivo

Aplicar y comparar diferentes tÃ©cnicas de validaciÃ³n cruzada y mÃ©tricas de evaluaciÃ³n sobre un modelo predictivo, utilizando datos reales. El objetivo principal es interpretar y justificar los resultados obtenidos en base a mÃ©tricas de rendimiento como:

- PrecisiÃ³n
- Recall
- F1-Score
- Matriz de ConfusiÃ³n
- Curvas ROC y Precision-Recall

---

## ğŸ§  Contexto del Problema

Se utiliza el dataset **Adult Census Income** (descargado desde [OpenML](https://www.openml.org/d/1590)) para simular un caso en que se desea predecir si un cliente abandonarÃ¡ un servicio. Para ello, se binariza la variable objetivo `class`:

- `>50K` â†’ `1.0` (simula "abandona")
- `<=50K` â†’ `0.0` (simula "permanece")

---

## ğŸ§¹ Preprocesamiento

Se realiza un preprocesamiento modular sobre el dataset:

- EliminaciÃ³n de valores faltantes (`NaN`)
- ConversiÃ³n de la variable objetivo a tipo binario (`float`)
- CodificaciÃ³n de variables categÃ³ricas mediante One-Hot Encoding
- EstandarizaciÃ³n de variables numÃ©ricas con `StandardScaler`

> âš ï¸ No se aplica `train_test_split` ya que se evaluarÃ¡ completamente con tÃ©cnicas de validaciÃ³n cruzada.

---

## ğŸ” TÃ©cnicas de ValidaciÃ³n Cruzada Aplicadas

El proyecto contempla mÃºltiples tÃ©cnicas, ajustadas al contexto del dataset:

- **K-Fold Cross Validation**
- **Leave-One-Out (LOO)**
- **Stratified K-Fold** (para mantener proporciones en datasets desbalanceados)
- *(Time Series CV descartado, pues el dataset no tiene estructura temporal)*

---

## ğŸ“Š Modelos Utilizados

- **Random Forest**
- **Logistic Regression**

Estos modelos serÃ¡n evaluados mediante todas las tÃ©cnicas de validaciÃ³n cruzada descritas.

---

## ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n

Se reportarÃ¡n y compararÃ¡n las siguientes mÃ©tricas:

- Accuracy
- Recall
- F1-Score
- Matriz de ConfusiÃ³n
- ROC AUC
- Precision-Recall AUC

---

## ğŸ“¦ Estructura del Proyecto

```

validacion\_cruzada/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils.py           # Carga y preprocesamiento
â”‚   â”œâ”€â”€ validacion.py         # Funciones para validaciÃ³n cruzada
â”‚   â”œâ”€â”€ entrenamiento.py      # Entrenamiento y evaluaciÃ³n de modelos
â”‚   â”œâ”€â”€ visualizacion.py      # GrÃ¡ficos: matriz, curvas, barras
â”‚
â”œâ”€â”€ main.py                   # Ejecuta todo el pipeline
â”œâ”€â”€ outputs/                  # Carpeta donde se guardan las figuras
â””â”€â”€ README.md                 # Este archivo

````

---

## ğŸ› ï¸ Requisitos

- Python 3.8+
- scikit-learn
- pandas
- seaborn
- matplotlib

InstalaciÃ³n recomendada:

```bash
conda activate especialidadmachinelearning
````

---

## âœ… Estado Actual

* [x] Dataset cargado y preprocesado
* [x] Objetivo binarizado correctamente
* [x] Eliminado `train_test_split` (se usarÃ¡ validaciÃ³n cruzada)
* [ ] ValidaciÃ³n cruzada implementada (prÃ³ximo paso)
* [ ] Visualizaciones agregadas
* [ ] AnÃ¡lisis de resultados

---

## ğŸ“¬ Autores

Trabajo desarrollado en modalidad grupal como parte de la especializaciÃ³n en Machine Learning.
Tiempo estimado de ejecuciÃ³n: 120 minutos.


---

## ğŸ“Š Resultados y EvaluaciÃ³n Comparativa de Modelos

Se evaluaron los modelos **RandomForest** y **LogisticRegression** utilizando dos estrategias de validaciÃ³n cruzada: `KFold` y `StratifiedKFold`. Para cada combinaciÃ³n se calcularon mÃ©tricas de desempeÃ±o y se generaron grÃ¡ficas de evaluaciÃ³n detalladas.

### ğŸ” ComparaciÃ³n General de MÃ©tricas

La figura a continuaciÃ³n muestra un resumen de las mÃ©tricas (`accuracy`, `precision`, `recall`, `f1_score`) promediadas por cada estrategia:

![ComparaciÃ³n de MÃ©tricas](outputs/comparacion_metricas_validacion.png)

Se observa un rendimiento muy similar entre ambas tÃ©cnicas, con ligeras ventajas en `f1_score` y `recall` usando `StratifiedKFold`, que conserva mejor la proporciÃ³n entre clases durante la particiÃ³n.

---

### ğŸ” Matrices de ConfusiÃ³n

Las siguientes grÃ¡ficas muestran la distribuciÃ³n de verdaderos positivos, negativos, falsos positivos y falsos negativos para cada modelo:

* **LogisticRegression + KFold**
  ![Matriz ConfusiÃ³n](outputs/matriz_confusion_LogisticRegression_KFold.png)
* **LogisticRegression + StratifiedKFold**
  ![Matriz ConfusiÃ³n](outputs/matriz_confusion_LogisticRegression_StratifiedKFold.png)
* **RandomForest + KFold**
  ![Matriz ConfusiÃ³n](outputs/matriz_confusion_RandomForest_KFold.png)
* **RandomForest + StratifiedKFold**
  ![Matriz ConfusiÃ³n](outputs/matriz_confusion_RandomForest_StratifiedKFold.png)

---

### ğŸ“ˆ Curvas Precision-Recall

Estas curvas permiten analizar la relaciÃ³n entre `precision` y `recall` para distintos umbrales de decisiÃ³n:

* LogisticRegression + KFold
  ![PR-Curve](outputs/precision_recall_LogisticRegression_KFold.png)
* LogisticRegression + StratifiedKFold
  ![PR-Curve](outputs/precision_recall_LogisticRegression_StratifiedKFold.png)
* RandomForest + KFold
  ![PR-Curve](outputs/precision_recall_RandomForest_KFold.png)
* RandomForest + StratifiedKFold
  ![PR-Curve](outputs/precision_recall_RandomForest_StratifiedKFold.png)

---

## âš ï¸ ExclusiÃ³n de Leave-One-Out (LOOCV)

Inicialmente se considerÃ³ la estrategia `LeaveOneOut`, pero fue descartada en etapas tempranas debido a:

* **Costos computacionales excesivos**: el dataset tiene mÃ¡s de 48.000 muestras. LOOCV genera **una iteraciÃ³n por cada observaciÃ³n**, lo que resulta en mÃ¡s de 48.000 ciclos de entrenamiento y evaluaciÃ³n.
* **Riesgo de overfitting**: LOOCV puede tener alta varianza en datasets grandes y desbalanceados, generando mÃ©tricas inconsistentes.
* **Resultados no representativos**: en pruebas iniciales, muchas mÃ©tricas resultaban nulas o indefinidas (ver advertencias `UndefinedMetricWarning`), especialmente cuando los folds contenÃ­an solo ejemplos de una clase.

Por estos motivos, **se optÃ³ por estrategias mÃ¡s eficientes y robustas como `KFold` y `StratifiedKFold`**, que permitieron conservar balance y reducir los tiempos de ejecuciÃ³n drÃ¡sticamente.

---

AquÃ­ tienes una secciÃ³n redactada en **Markdown** para tu informe final, que incluye:

* Resultados generales
* JustificaciÃ³n de la exclusiÃ³n de Leave-One-Out (LOOCV)
* Referencias a las imÃ¡genes generadas.


---

### ğŸ§ª Curvas ROC (Receiver Operating Characteristic)

Las curvas ROC muestran la tasa de verdaderos positivos contra la tasa de falsos positivos. En este anÃ¡lisis, todos los modelos alcanzaron un **AUC (Area Under Curve) de aproximadamente 0.90**, lo que indica un buen rendimiento.

#### RandomForest

![ROC KFold](outputs/roc_curve_RandomForest_KFold.png)
![ROC StratifiedKFold]outputs/(roc_curve_RandomForest_StratifiedKFold.png)

#### LogisticRegression

![ROC KFold](outputs/roc_curve_LogisticRegression_KFold.png)
![ROC StratifiedKFold](outputs/roc_curve_LogisticRegression_StratifiedKFold.png)

---

## âœ… Conclusiones

* **StratifiedKFold** fue ligeramente mÃ¡s robusto que KFold en tÃ©rminos de mÃ©tricas debido al manejo equilibrado de clases.
* **RandomForest** y **LogisticRegression** presentaron rendimientos muy similares, aunque RandomForest tuvo una ligera ventaja en `recall` y `f1_score`.
* La exclusiÃ³n de **LOOCV** fue una decisiÃ³n justificada por razones de eficiencia computacional y confiabilidad de mÃ©tricas.

