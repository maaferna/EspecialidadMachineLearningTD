# üîé Resultados Autoencoders (MNIST)

## üìå Objetivo
Comparar el desempe√±o de:
- **Autoencoder b√°sico (AE)**: reconstrucci√≥n directa de im√°genes MNIST.
- **Autoencoder con denoising (DAE)**: entrenado con ruido gaussiano (œÉ=0.5) para robustez.

El fin es analizar c√≥mo la adici√≥n de ruido afecta la reconstrucci√≥n, la estabilidad del entrenamiento y la capacidad de generalizaci√≥n del modelo.

---

## üìä Resultados de entrenamiento

### AE b√°sico
- **Curvas de entrenamiento**  
  ![Curvas Basic AE](outputs_ae/ae_basic__lr0.001__bs128__ep50_curves.png)

- **Reconstrucciones**  
  ![Reconstrucciones Basic AE](outputs_ae/ae_basic__lr0.001__bs128__ep50_recons.png)

- **M√©tricas finales (√©poca 50)**  
  - Train loss: ~0.0149  
  - Val loss: ~0.0095  
  - MAE val: ~0.032  

‚úî Buen ajuste y convergencia estable.  
‚ö† Posible **sobreajuste leve**: la brecha train/val se reduce pero no desaparece.

---

### AE con Denoising (œÉ=0.5)
- **Curvas de entrenamiento**  
  ![Curvas Denoising AE](outputs_ae/ae_denoise__lr0.001__bs128__ep50__sigma0.5_curves.png)

- **Reconstrucciones (denoising)**  
  ![Reconstrucciones Denoising AE](outputs_ae/ae_denoise__lr0.001__bs128__ep50__sigma0.5_denoise_recons.png)

- **M√©tricas finales (√©poca 50)**  
  - Train loss: ~0.0204  
  - Val loss: ~0.0172  
  - MAE val: ~0.047  

‚úî Logra **recuperar estructura de d√≠gitos aun con ruido fuerte**.  
‚ö† Coste: mayor error absoluto (MAE ‚Üë) respecto al AE b√°sico, lo cual es esperado.

---

## üìà Comparaci√≥n

- **AE b√°sico** alcanza menor error de reconstrucci√≥n absoluto.  
- **DAE** sacrifica exactitud pixel a pixel, pero **generaliza mejor a datos corruptos**, manteniendo legibilidad de d√≠gitos.  
- En tareas pr√°cticas (ej. preprocesamiento o reducci√≥n de ruido en im√°genes reales), el **DAE es preferible**.

---

## ü§î Reflexi√≥n
1. **Eficiencia computacional**: ambos entrenan r√°pido con batch=128 en GPU (4090), ocupando ~2‚Äì3 GB VRAM.  
2. **Robustez**: el denoising AE demuestra la importancia de introducir ruido en entrenamiento para entornos con se√±ales degradadas.  
3. **Siguientes pasos**:
   - Extender a **Convolutional Autoencoders (Conv-AE)** para aprovechar la estructura espacial.  
   - Explorar **variational autoencoders (VAE)** para generaci√≥n sint√©tica.  
   - Ajustar **œÉ din√°mico** (curriculum de ruido) para mejorar robustez sin sacrificar tanto MAE.

---

# üîé Resultados Autoencoders ‚Äî Comparaci√≥n 20 vs 50 √©pocas

## üìå Objetivo
Evaluar c√≥mo cambia el desempe√±o de un **Autoencoder b√°sico (AE)** y un **Autoencoder con denoising (DAE, œÉ=0.5)** cuando se entrenan por **20 vs. 50 √©pocas**, en t√©rminos de reconstrucci√≥n, convergencia y robustez al ruido.

---

## üìä Resultados AE B√°sico

### 20 √©pocas
- **Curvas de entrenamiento**  
  ![Curvas Basic AE 20](outputs_ae/ae_basic__lr0.001__bs128__ep20_curves.png)

- **Reconstrucciones**  
  ![Reconstrucciones Basic AE 20](outputs_ae/ae_basic__lr0.001__bs128__ep20_recons.png)

- **M√©tricas finales (√©poca 20)**  
  - Val loss: ~0.0172  
  - Val MAE: ~0.0478  

‚úî Aprendizaje r√°pido en pocas √©pocas.  
‚ö† Menor precisi√≥n de reconstrucci√≥n comparado con 50 √©pocas.

---

### 50 √©pocas
- **Curvas de entrenamiento**  
  ![Curvas Basic AE 50](outputs_ae/ae_basic__lr0.001__bs128__ep50_curves.png)

- **Reconstrucciones**  
  ![Reconstrucciones Basic AE 50](outputs_ae/ae_basic__lr0.001__bs128__ep50_recons.png)

- **M√©tricas finales (√©poca 50)**  
  - Val loss: ~0.0095  
  - Val MAE: ~0.032  

‚úî Reconstrucciones mucho m√°s n√≠tidas.  
‚ö† Indicios leves de sobreajuste (gap train/val).

---

## üìä Resultados AE con Denoising (œÉ=0.5)

### 20 √©pocas
- **Curvas de entrenamiento**  
  ![Curvas Denoising AE 20](outputs_ae/ae_denoise__lr0.001__bs128__ep20__sigma0.5_curves.png)

- **Reconstrucciones**  
  ![Reconstrucciones Denoising AE 20](outputs_ae/ae_denoise__lr0.001__bs128__ep20__sigma0.5_denoise_recons.png)

- **M√©tricas finales (√©poca 20)**  
  - Val loss: ~0.0172  
  - Val MAE: ~0.0473  

‚úî Capacidad inicial de limpiar ruido.  
‚ö† Denoise parcial: todav√≠a borroso en algunos d√≠gitos.

---

### 50 √©pocas
- **Curvas de entrenamiento**  
  ![Curvas Denoising AE 50](outputs_ae/ae_denoise__lr0.001__bs128__ep50__sigma0.5_curves.png)

- **Reconstrucciones**  
  ![Reconstrucciones Denoising AE 50](outputs_ae/ae_denoise__lr0.001__bs128__ep50__sigma0.5_denoise_recons.png)

- **M√©tricas finales (√©poca 50)**  
  - Val loss: ~0.0172  
  - Val MAE: ~0.047  

‚úî Reconstrucciones m√°s consistentes que en 20 √©pocas.  
‚ö† El error se mantiene mayor al AE b√°sico, lo cual es esperado porque debe lidiar con ruido adicional.

---

## üìà Comparaci√≥n General

- **20 √©pocas**:  
  - AE b√°sico ya reconstruye aceptablemente, pero el denoise a√∫n muestra limitaciones.  
  - Ideal para entrenamiento r√°pido y prototipado.  

- **50 √©pocas**:  
  - AE b√°sico logra **alta fidelidad visual** (menor MAE).  
  - DAE mantiene **robustez al ruido**, sacrificando algo de detalle.  
  - Ambos modelos alcanzan convergencia estable.  

---

## ü§î Reflexi√≥n
- El **n√∫mero de √©pocas impacta fuertemente** al AE b√°sico (aprende m√°s detalle con m√°s tiempo).  
- En el **DAE**, la diferencia entre 20 y 50 √©pocas es m√°s leve: el entrenamiento extra refina, pero el ruido siempre limita la reconstrucci√≥n perfecta.  
- **Aplicaci√≥n pr√°ctica**:  
  - Para compresi√≥n pura ‚Üí AE b√°sico + mayor n√∫mero de √©pocas.  
  - Para limpieza/robustez en datos ruidosos ‚Üí DAE incluso con menos √©pocas.


---


# üîç An√°lisis Comparativo y Reflexi√≥n Final

## üìä Comparaci√≥n entre AE B√°sico y Denoising AE

En t√©rminos cuantitativos, el **Autoencoder b√°sico (AE)** logra **p√©rdidas de reconstrucci√≥n menores** y un **MAE m√°s bajo** (‚âà0.032 tras 50 √©pocas) en comparaci√≥n con el **Denoising Autoencoder (DAE)** (‚âà0.047 con ruido œÉ=0.5). Esto se traduce en reconstrucciones m√°s n√≠tidas y detalladas en el AE cl√°sico, especialmente cuando se entrena durante m√°s √©pocas.  

En cambio, el **DAE** muestra reconstrucciones menos precisas desde el punto de vista visual, pero con la **ventaja de eliminar ruido** y mantener estructuras reconocibles en escenarios adversos. A nivel visual, el AE b√°sico sobresale cuando los datos no contienen ruido significativo, mientras que el DAE ofrece mayor robustez y estabilidad frente a perturbaciones externas.  

En resumen:
- **AE B√°sico**: mejor fidelidad visual, menor p√©rdida de reconstrucci√≥n.  
- **DAE**: mejor tolerancia al ruido, aunque sacrifica detalles.  
- **Aplicaci√≥n pr√°ctica**: la elecci√≥n depende del contexto; en entornos controlados se prefiere AE, en entornos ruidosos o inseguros, DAE.

---

## ü§î Reflexi√≥n sobre aplicaciones (‚âà230 palabras)

Los autoencoders, tanto b√°sicos como de denoising, tienen un enorme potencial en **medicina, seguridad e industria**. En medicina, los **DAE** podr√≠an utilizarse para mejorar la calidad de im√°genes m√©dicas ruidosas como radiograf√≠as, resonancias magn√©ticas o tomograf√≠as, permitiendo diagn√≥sticos m√°s confiables incluso en condiciones de baja calidad de captura. El **AE b√°sico**, por otro lado, puede emplearse en compresi√≥n de im√°genes m√©dicas, reduciendo almacenamiento y costos de transmisi√≥n de datos sin perder informaci√≥n cl√≠nica cr√≠tica.

En el √°mbito de la **seguridad**, los autoencoders permiten detectar anomal√≠as en video vigilancia. Por ejemplo, un modelo entrenado con grabaciones de entornos normales puede identificar actividades sospechosas al detectar desviaciones en la reconstrucci√≥n. En ciberseguridad, pueden emplearse para identificar patrones inusuales en tr√°fico de red, ayudando a prevenir ataques.

En la **industria**, estas t√©cnicas facilitan mantenimiento predictivo. Un AE entrenado con se√±ales de m√°quinas en buen estado puede se√±alar fallas incipientes cuando la reconstrucci√≥n del estado real diverge de lo esperado. Adem√°s, el denoising AE puede limpiar se√±ales de sensores industriales ruidosos, mejorando la confiabilidad en tiempo real.

En conclusi√≥n, mientras que el **AE b√°sico** es ideal para compresi√≥n y representaci√≥n eficiente, el **DAE** aporta robustez y capacidad de operar en ambientes adversos. Ambos modelos, correctamente aplicados, son herramientas valiosas que pueden transformar la forma en que distintas √°reas procesan y entienden la informaci√≥n.


