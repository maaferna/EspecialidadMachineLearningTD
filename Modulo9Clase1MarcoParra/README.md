# Interpretando Modelos de Clasificaci√≥n de Opiniones con LIME y SHAP

**Objetivo**: Entrenar un clasificador binario (opiniones positivo/negativo) y explicar sus predicciones con **LIME** y **SHAP**.  

## Estructura

# üìò README ‚Äî Interpretando Modelos de Clasificaci√≥n de Opiniones con LIME y SHAP

## üéØ Objetivo

Aplicar t√©cnicas de explicabilidad (**LIME** y **SHAP**) para comprender y justificar las decisiones de un modelo de clasificaci√≥n binaria de opiniones (positivo/negativo).
El fin es desarrollar criterio t√©cnico y √©tico sobre c√≥mo influyen las variables en el proceso de clasificaci√≥n.

---

## ‚öôÔ∏è Metodolog√≠a

1. **Dataset**: Opiniones en texto (IMDb Small). Etiquetas binarias ‚Üí `neg` / `pos`.
2. **Modelo**: Clasificador lineal (Logistic Regression) entrenado con `TfidfVectorizer`.
3. **Explicabilidad**:

   * **LIME**: interpretaciones locales para 5 ejemplos ‚Üí exportado en **HTML** y **PNG**.
   * **SHAP**: explicaciones con `KernelExplainer` para los mismos ejemplos ‚Üí exportado en **PNG**.
4. **Evaluaci√≥n**: Matriz de confusi√≥n + m√©tricas (precision, recall, F1, accuracy).

---

## üìä Resultados

### üîπ M√©tricas globales

Accuracy y F1 en torno al **85%**, lo que indica un desempe√±o robusto pero con margen de mejora.

```json
{
  "accuracy": 0.85,
  "f1_macro": 0.85,
  "report": {
    "neg": {"precision": 0.87, "recall": 0.83, "f1-score": 0.85},
    "pos": {"precision": 0.83, "recall": 0.87, "f1-score": 0.85}
  }
}
```

---

### üîπ Matriz de confusi√≥n

![Confusion Matrix](reports/confusion_matrix.png)

* **neg**: 258 bien clasificados, 52 falsos positivos.
* **pos**: 253 bien clasificados, 37 falsos negativos.

---

### üîπ Ejemplos de explicabilidad

#### LIME

![LIME Ejemplo 0](reports/figures/explainability/lime_ex_0.png)
![LIME Ejemplo 1](reports/figures/explainability/lime_ex_1.png)
![LIME Ejemplo 2](reports/figures/explainability//lime_ex_2.png)
![LIME Ejemplo 3](reports/figures/explainability//lime_ex_3.png)
![LIME Ejemplo 4](reports/figures/explainability//lime_ex_4.png)

#### SHAP

![SHAP Ejemplo 0](reports/figures/explainability//shap_ex_0.png)
![SHAP Ejemplo 1](reports/figures/explainability//shap_ex_1.png)
![SHAP Ejemplo 2](reports/figures/explainability//shap_ex_2.png)

---

## üîç Comparaci√≥n LIME vs SHAP

* **Coincidencia**: ambos identifican tokens clave (‚Äúbad‚Äù, ‚Äúcrap‚Äù, ‚Äúfun‚Äù, ‚Äúamazing‚Äù).
* **LIME**: m√°s intuitivo ‚Üí muestra pesos locales directamente vinculados a cada predicci√≥n.
* **SHAP**: m√°s consistente en el marco global ‚Üí cuantifica importancia relativa de cada palabra.

---

## üí° Reflexi√≥n cr√≠tica

* **Comportamiento del modelo**: responde bien a tokens fuertes, pero puede fallar en textos ambiguos o ir√≥nicos.
* **T√©cnica m√°s √∫til**:

  * **LIME** ‚Üí claridad visual y r√°pida exploraci√≥n.
  * **SHAP** ‚Üí m√°s riguroso y matem√°ticamente consistente.
* **Aspectos √©ticos**:

  * Riesgo de **sobreinterpretar tokens sensibles** (ej. palabras de g√©nero o salud).
  * Necesidad de **auditar sesgos** y garantizar explicaciones comprensibles para usuarios finales.

---

## ‚úÖ Checklist de la consigna

‚úî Dataset peque√±o con opiniones etiquetadas.
‚úî Entrenamiento de modelo binario (Logistic Regression).
‚úî Explicaciones con **LIME** (‚â•2 instancias).
‚úî Explicaciones con **SHAP** (‚â•2 instancias).
‚úî Comparaci√≥n cr√≠tica entre LIME y SHAP.
‚úî Reflexi√≥n t√©cnica + √©tica.

---
