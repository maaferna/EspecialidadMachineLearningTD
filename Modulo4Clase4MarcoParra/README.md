# ğŸ§¬ Proyecto: Ajuste de HiperparÃ¡metros con Algoritmos GenÃ©ticos

Este proyecto aplica un algoritmo genÃ©tico para optimizar los hiperparÃ¡metros de un modelo RandomForest, utilizando el dataset de cÃ¡ncer de mama incluido en .

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ data/                # Archivos persistentes si fuera necesario
â”œâ”€â”€ notebooks/           # Jupyter notebooks generados
â”œâ”€â”€ outputs/             # GrÃ¡ficos y mÃ©tricas generadas
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ main.py          # Pipeline de entrenamiento
â”‚   â””â”€â”€ crear_notebook.py# Generador automÃ¡tico de notebook
â”œâ”€â”€ src/
â”œ   â”œâ”€ optimizador.py      # Optimizadores
â”‚   â”œâ”€â”€ optimizador.py      # LÃ³gica de optimizacion
â”‚   â”œâ”€â”€ modelos.py       # Modelo base y optimizado
â”‚   â”œâ”€â”€ utils.py         # Preprocesamiento y carga
â”‚   â””â”€â”€ visualizador.py  # VisualizaciÃ³n de mÃ©tricas
â”œâ”€â”€ environment.yml      # Entorno compartido con la especialidad
â””â”€â”€ README.md
```

## ğŸš€ Instrucciones para Ejecutar

1. Activar entorno:
```bash
conda activate especialidadmachinelearning
```

2. Ejecutar el pipeline:
```bash
python -m scripts.main
```

3. Generar el notebook:
```bash
python -m scripts.crear_notebook
```

## ğŸ§  Objetivo

Optimizar los hiperparÃ¡metros de `RandomForestClassifier` usando un **algoritmo genÃ©tico** con la librerÃ­a deap y comparar los resultados con modelos de optimizaciÃ²n tradicionales.


Optimizar modelos predictivos de enfermedades crÃ³nicas mediante diferentes estrategias de bÃºsqueda de hiperparÃ¡metros, con foco en el rendimiento evaluado por mÃ©tricas como AUC, F1, PrecisiÃ³n y Recall.

---

## ğŸ“ˆ ComparaciÃ³n de MÃ©tricas por Modelo

![ComparaciÃ³n de mÃ©tricas](outputs/comparacion_metricas_modelos.png)

---

## ğŸ“Š Resultados por MÃ©todo

| Modelo         | ROC Curve                      | Matriz de ConfusiÃ³n               |
|----------------|--------------------------------|-----------------------------------|
| Base           | ![](outputs/roc_base.png)      | ![](outputs/matriz_confusion_base.png) |
| GenÃ©tico       | ![](outputs/roc_genÃ©tico.png)  | ![](outputs/matriz_confusion_genÃ©tico.png) |
| GridSearch     | ![](outputs/roc_gridsearch.png)| ![](outputs/matriz_confusion_gridsearch.png) |
| RandomSearch   | ![](outputs/roc_randomsearch.png)| ![](outputs/matriz_confusion_randomsearch.png) |
| Optuna         | ![](outputs/roc_optuna.png)    | ![](outputs/matriz_confusion_optuna.png) |
| Skopt          | ![](outputs/roc_skopt.png)     | ![](outputs/matriz_confusion_skopt.png) |

---

## ğŸ” AnÃ¡lisis de Resultados

- **PrecisiÃ³n general**: todos los modelos optimizados mantuvieron un AUC de 0.99, lo cual indica que la capacidad discriminativa se mantuvo alta en todos los casos.

- **F1-Score y Recall**: el modelo optimizado con **Optuna** fue el que logrÃ³ el mejor balance entre precisiÃ³n (0.95) y recall (0.97), alcanzando un F1-score de 0.96, ideal para contextos donde los falsos negativos tienen alto costo.

- **Modelos GenÃ©tico, Grid y RandomSearch** ofrecieron resultados estables y prÃ¡cticamente equivalentes en mÃ©tricas principales, lo que refleja que incluso mÃ©todos clÃ¡sicos siguen siendo competitivos.

- **Scikit-Optimize (Skopt)** resultÃ³ muy eficiente, manteniendo todas las mÃ©tricas en 0.94â€“0.99, y representa una alternativa liviana para exploraciÃ³n rÃ¡pida.

- **Errores de clasificaciÃ³n**: observando las matrices de confusiÃ³n, la cantidad de falsos positivos y falsos negativos se mantuvo baja y equilibrada en todos los mÃ©todos. Optuna logrÃ³ reducir los falsos negativos a solo 3 casos.

El uso de tÃ©cnicas de optimizaciÃ³n permite refinar significativamente modelos base en problemas de clasificaciÃ³n multiclase complejos. Herramientas como Optuna y Ray Tune permiten explorar espacios de bÃºsqueda de manera mÃ¡s eficiente que los mÃ©todos tradicionales, reduciendo tiempos y mejorando resultados. En contextos sensibles como la salud pÃºblica, maximizar el recall es crucial, y este anÃ¡lisis muestra cÃ³mo la optimizaciÃ³n puede lograrlo sin comprometer la precisiÃ³n.

---

