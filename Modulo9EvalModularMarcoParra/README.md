# Proyecto: Interpretabilidad de Modelos Predictivos con LIME y SHAP

## üìå Resumen Ejecutivo

Este proyecto aborda la **interpretabilidad de modelos de clasificaci√≥n** aplicados al dataset *Heart Failure Prediction (Kaggle)*.
El objetivo fue no solo entrenar un modelo con buen desempe√±o, sino tambi√©n **explicar sus predicciones** utilizando t√©cnicas modernas de explicabilidad: **SHAP** y **LIME**.

Los resultados muestran que el modelo Random Forest alcanza una **Accuracy del 90,8% y un F1-score del 91,9%**. Mediante SHAP y LIME se identificaron las variables m√°s influyentes en las decisiones del modelo, destacando **ST\_Slope**, **ChestPainType** y **Cholesterol**, junto con el an√°lisis de variables sensibles como `Sex`.

---

## üéØ Objetivos

1. Entrenar un modelo de clasificaci√≥n (Random Forest) sobre el dataset *Heart Failure Prediction*.
2. Evaluar su desempe√±o con m√©tricas est√°ndar (Accuracy, F1-score, Classification Report).
3. Aplicar **SHAP** para obtener explicaciones globales y locales de las predicciones.
4. Aplicar **LIME** para generar explicaciones locales en instancias espec√≠ficas.
5. Analizar posibles **sesgos y riesgos √©ticos**, en particular asociados a variables sensibles.
6. Proponer mejoras al modelo y al flujo de trabajo para favorecer decisiones m√°s responsables.

---

## üîß Metodolog√≠a

* **Carga y limpieza:** eliminaci√≥n de valores nulos y casting de variables categ√≥ricas.
* **Modelo:** Pipeline con preprocesamiento (`StandardScaler`, `OneHotEncoder`) y clasificador `RandomForestClassifier`.
* **Evaluaci√≥n:** Divisi√≥n train/test estratificada (80/20), m√©tricas de desempe√±o.
* **Interpretabilidad:**

  * **SHAP:** summary plots y waterfall plots de 3 casos.
  * **LIME:** explicaciones locales para los mismos 3 casos.
* **√âtica:** reporte de importancia media de SHAP y an√°lisis de variables sensibles.

---

## üìä Resultados

### M√©tricas de evaluaci√≥n

```
Accuracy: 0.908
F1-score: 0.919

Classification report:
              precision    recall  f1-score   support
           0      0.922     0.866     0.893        82
           1      0.897     0.941     0.919       102
```

### Explicaciones SHAP

* **Globales:**
  ![SHAP Beeswarm](images/shap_summary_beeswarm.png)
  ![SHAP Bar](images/shap_summary_bar.png)

* **Locales (3 casos):**
  ![SHAP Waterfall 1](images/shap_waterfall_case_1.png)
  ![SHAP Waterfall 2](images/shap_waterfall_case_2.png)
  ![SHAP Waterfall 3](images/shap_waterfall_case_3.png)

### Explicaciones LIME

* **Locales (3 casos):**
  ![LIME 1](images/lime_explanation_case_1.png)
  ![LIME 2](images/lime_explanation_case_2.png)
  ![LIME 3](images/lime_explanation_case_3.png)

---

## ‚öñÔ∏è An√°lisis √©tico y de sesgo

* SHAP evidenci√≥ que variables como `Sex_F` y `Sex_M` tienen un peso no despreciable (‚âà0.02‚Äì0.025).
* Esto sugiere un **potencial sesgo de g√©nero** en la predicci√≥n.
* Riesgo: decisiones cl√≠nicas injustas si el modelo se aplica sin control.
* Mitigaci√≥n: balancear dataset, excluir variables sensibles, usar t√©cnicas de fairness (post-procesamiento de predicciones).

---

## üöÄ Propuestas de mejora

* Evaluar algoritmos alternativos como **XGBoost** y comparar m√©tricas + explicaciones.
* Ajustar el preprocesamiento (outliers, normalizaci√≥n m√°s robusta).
* Entrenar versiones del modelo **sin variables sensibles** y analizar el impacto.

---


