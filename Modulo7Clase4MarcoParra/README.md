# ğŸŒ¸ Proyecto: Transfer Learning con ResNet50 y EfficientNetB0

## ğŸ“Œ Objetivo

Aplicar **Transfer Learning** con arquitecturas modernas (ResNet50 y EfficientNetB0) para resolver tareas de **clasificaciÃ³n de imÃ¡genes reales**.
El flujo completo incluye:

1. Carga y preprocesamiento de datos (`flower_photos` y `CIFAR-10`).
2. AdaptaciÃ³n del modelo base (capas adicionales).
3. Entrenamiento y validaciÃ³n.
4. EvaluaciÃ³n cuantitativa y visualizaciÃ³n de resultados.

---

## ğŸ“ Resumen Ejecutivo

Este proyecto demuestra cÃ³mo aprovechar modelos preentrenados en ImageNet para tareas de clasificaciÃ³n de imÃ¡genes en datasets reales.
Se compararon dos arquitecturas:

* **ResNet50** sobre el dataset de **flores**.
* **EfficientNetB0** sobre **CIFAR-10**.

Resultados destacados:

* ResNet50 alcanzÃ³ una **accuracy de validaciÃ³n >91%** en *flower\_photos*.
* EfficientNetB0 logrÃ³ **\~92% en validaciÃ³n** en *CIFAR-10*.
* La fine-tuning de ResNet50 mejorÃ³ la capacidad de generalizaciÃ³n al permitir que capas profundas tambiÃ©n ajustaran pesos.

---

## âš™ï¸ InstalaciÃ³n y Dependencias

### 1ï¸âƒ£ Crear entorno

```bash
conda create -n tf_transfer python=3.10
conda activate tf_transfer
```

### 2ï¸âƒ£ Instalar dependencias

```bash
pip install tensorflow matplotlib seaborn scikit-learn
```

### 3ï¸âƒ£ Descargar datasets

* **flowers**:

```bash
bash scripts/get_flowers.sh
```

* **CIFAR-10** se descarga automÃ¡ticamente desde `keras.datasets`.

---

## â–¶ï¸ EjecuciÃ³n

### Entrenar con ResNet50 en *flowers*

```bash
python -m scripts.main --dataset flowers --base resnet50 --epochs 50 --batch-size 32 --img-size 224
```

### Entrenar con EfficientNetB0 en *CIFAR-10*

```bash
python -m scripts.main --dataset cifar10 --base efficientnetb0 --epochs 50 --batch-size 64 --img-size 224
```

### Fine-Tuning de ResNet50 (descongelar capas profundas)

```bash
python -m scripts.main --dataset flowers --base resnet50 --epochs 50 --batch-size 32 --img-size 224 --finetune
```

---

## ğŸ“Š Resultados

### ğŸ”¹ ResNet50 en *flowers*

* **Curvas de entrenamiento**
  ![ResNet50 Curves](outputs_tl/resnet50__flowers__img224__bs32__ep50_curves.png)

* **Matriz de confusiÃ³n**
  ![ResNet50 Confusion Matrix](outputs_tl/resnet50__flowers__img224__bs32__ep50_cm.png)

* **Predicciones vs reales**
  ![ResNet50 Predictions](outputs_tl/resnet50__flowers__img224__bs32__ep50_pred_grid.png)

---

### ğŸ”¹ EfficientNetB0 en *CIFAR-10*

* **Curvas de entrenamiento**
  ![EfficientNet Curves](outputs_tl/efficientnetb0__cifar10__img224__bs64__ep50_curves.png)

* **Matriz de confusiÃ³n**
  ![EfficientNet Confusion Matrix](outputs_tl/efficientnetb0__cifar10__img224__bs64__ep50_cm.png)

* **Predicciones vs reales**
  ![EfficientNet Predictions](outputs_tl/efficientnetb0__cifar10__img224__bs64__ep50_pred_grid.png)

---

### ğŸ”¹ Fine-Tuning ResNet50

* **Curvas de entrenamiento (Fine-Tune)**
  ![FT ResNet50 Curves](outputs_tl/resnet50__flowers__img224__bs32__ep50__ft_curves.png)

* **Matriz de confusiÃ³n**
  ![FT ResNet50 CM](outputs_tl/resnet50__flowers__img224__bs32__ep50__ft_cm.png)

* **Predicciones vs reales**
  ![FT ResNet50 Predictions](outputs_tl/resnet50__flowers__img224__bs32__ep50__ft_pred_grid.png)

---

## ğŸ“Œ AnÃ¡lisis CrÃ­tico

### â“ Â¿Por quÃ© se eligiÃ³ esa arquitectura?

* **ResNet50**: probada en visiÃ³n por computadora, buen balance entre profundidad y eficiencia.
* **EfficientNetB0**: diseÃ±ada para escalar eficientemente parÃ¡metros y FLOPs, ideal para CIFAR-10.

### âš ï¸ Principales desafÃ­os

* **Preprocesamiento**: necesario adaptar el tamaÃ±o de las imÃ¡genes a 224x224 para ambos modelos.
* **Overfitting**: evidente en algunas curvas; mitigado con *data augmentation* y *fine-tuning*.
* **Computo**: entrenamiento completo requiere GPU con suficiente memoria.

### ğŸ”§ Mejoras para producciÃ³n

* Aplicar **regularizaciÃ³n adicional** (Dropout, L2).
* **Fine-tuning progresivo** (descongelar capas en bloques).
* Implementar **monitorizaciÃ³n en tiempo real** (TensorBoard).
