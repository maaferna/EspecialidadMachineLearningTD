# ClasificaciÃ³n de notas clÃ­nicas con enfoque Ã©tico â€” README

## ğŸ¯ Resumen ejecutivo

Este proyecto implementa un **pipeline de NLP** para clasificar **notas clÃ­nicas** por **gravedad** (`0=leve`, `1=moderado`, `2=severo`) utilizando:

* **Preprocesamiento** modular (limpieza, tokenizaciÃ³n, stopwords, lematizaciÃ³n).
* **VectorizaciÃ³n** con **TF-IDF**.
* **Modelos lineales** (SVM lineal o RegresiÃ³n LogÃ­stica).
* **EvaluaciÃ³n** en test con matriz de confusiÃ³n y mÃ©tricas macro.
* **AuditorÃ­a de sesgos** por atributo sensible (`sexo`) con mÃ©tricas por grupo.

Se evita **fuga de informaciÃ³n**:

* Textos **no** contienen palabras de clase.
* OpciÃ³n de **agrupamiento por plantilla** (`template_id`) para splits robustos.

âœ… Tipo de afecciÃ³n: estÃ¡ implementado en el dataset como template_id â†’ 1:gastritis_pauta, 2:cefalea_subita, etc. â†’ cumple con el requisito de metadatos asociados (tipo de afecciÃ³n).

Artefactos principales:

* Modelos/TF-IDF en `models/artifacts/`
* MÃ©tricas en `reports/metrics_cls.json`
* Figuras en `reports/figures/`
* AuditorÃ­a por grupo en `reports/fairness/metrics_by_group.json`

---

## âš™ï¸ Requisitos (Conda)

```bash
# 1) Crear y activar entorno
conda env create -f environment.yml
conda activate especialidadmachinelearning

# 2) Modelos/recursos (si no estaban instalados)
python -m spacy download es_core_news_sm
python - <<'PY'
import nltk
for pkg in ["punkt", "punkt_tab", "stopwords"]: 
    nltk.download(pkg)
PY
```

> Si usas GPUs/CUDA u otro stack, ajusta `environment.yml` segÃºn tu plataforma.

---

## ğŸ—‚ï¸ Estructura (simplificada)

```
.
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config_default.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/clinical_notes_labeled.csv   # (modo CSV)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ artifacts/
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ fairness/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ main_train.py         # entrenamiento + evaluaciÃ³n en test
â”‚   â”œâ”€â”€ main_infer.py         # inferencia puntual
â”‚   â”œâ”€â”€ evaluate_bias.py      # auditorÃ­a por grupo sensible
â”‚   â””â”€â”€ (opcional) generate_csv.py
â””â”€â”€ src/...
```

---

## ğŸ§ª Dataset

### OpciÃ³n A â€” Usar el CSV (recomendado)

Coloca tu archivo en:

```
data/raw/clinical_notes_labeled.csv
```

Debe tener columnas: `text`, `label` (0/1/2), `sexo`, (opcional `template_id`).

En `configs/config_default.yaml`:

```yaml
dataset:
  mode: csv
  csv_path: data/raw/clinical_notes_labeled.csv
  text_col: text
  label_col: label
  sensitive_col: sexo
  test_size: 0.3
  stratify: true
```

### OpciÃ³n B â€” Generar un CSV sintÃ©tico (â‰¥100 filas)

Ejecuta (ajusta la ruta si tu proyecto estÃ¡ en otra carpeta):

```bash
python - <<'PY'
import numpy as np, pandas as pd
from pathlib import Path
PROJECT_ROOT = Path(".").resolve()
out = PROJECT_ROOT / "data" / "raw"
out.mkdir(parents=True, exist_ok=True)
csv = out / "clinical_notes_labeled.csv"

MIN_TOTAL = 100
AUG = ["", " Reevaluar en 24 h.", " Control en 48 h.", " Realizar exÃ¡menes bÃ¡sicos.", " Reposo relativo.", " HidrataciÃ³n oral."]
T = {
  0:[("cefalea_mareo","Cefalea y mareo ocasional. Sin fiebre."),
     ("rinitis_controlada","Rinitis alÃ©rgica controlada con antihistamÃ­nicos."),
     ("molestia_gastrica","Molestia gÃ¡strica postprandial; indica dieta blanda."),
     ("dolor_lumbar","Dolor lumbar tras caminata prolongada; reposo relativo."),
     ("resfrio_comun","ResfrÃ­o con rinorrea y odinofagia."),
     ("hiperglicemia_control","Hiperglicemia en ayuno; plan de control nutricional."),
     ("dermatitis_leve","Dermatitis en antebrazos; sin signos de infecciÃ³n."),
     ("tension_cefalica","TensiÃ³n cefÃ¡lica intermitente; responde a analgÃ©sicos simples.")],
  1:[("diarrea_24h","Diarrea de 24 horas con malestar general. HidrataciÃ³n indicada."),
     ("tos_nocturna_asma","Tos seca nocturna; antecedente de asma. SaturaciÃ³n 96%."),
     ("lumbalgia_limitante","Lumbalgia con limitaciÃ³n funcional tras esfuerzo."),
     ("dermatitis_prurito","Dermatitis con prurito intenso y eritema."),
     ("crisis_asmatica","Crisis asmÃ¡tica con uso de inhalador de rescate."),
     ("dolor_abdominal_colico","Dolor abdominal tipo cÃ³lico, sin irritaciÃ³n peritoneal."),
     ("bronquitis_expectoracion","Bronquitis con expectoraciÃ³n mucoide."),
     ("gastritis_pauta","Gastritis; pauta con inhibidor de bomba y control.")],
  2:[("dolor_toracico_brazo","Dolor torÃ¡cico opresivo irradiado a brazo izquierdo."),
     ("disnea_edema","Disnea en reposo y edema de tobillos."),
     ("insomnio_impacto","Insomnio con impacto funcional diurno significativo."),
     ("hemorragia_digestiva","Melena y mareo al bipedestar; sospecha de sangrado digestivo."),
     ("sat_88","SaturaciÃ³n 88% al aire ambiente; cianosis peribucal."),
     ("fiebre_rigidez_nuca","Fiebre alta persistente con rigidez de nuca."),
     ("dolor_toracico_ecg","Dolor torÃ¡cico con diaforesis y nÃ¡useas; se solicita ECG."),
     ("cefalea_subita","Cefalea sÃºbita intensa con vÃ³mitos; descartar HSA.")],
}
rng = np.random.default_rng(42)
def sample_rows(npc=34):
    rows=[]
    for y,pairs in T.items():
        keys,bases=zip(*pairs); keys=np.array(keys); bases=np.array(bases)
        for i in range(npc):
            text=rng.choice(bases)+rng.choice(AUG)
            sexo="F" if i%2==0 else "M"
            k=rng.choice(keys)
            rows.append({"text":text,"label":int(y),"sexo":sexo,"template_id":f"{y}:{k}"})
    return rows

n_classes=len(T)
npc=int(np.ceil(MIN_TOTAL/n_classes))
df=pd.DataFrame(sample_rows(npc)).drop_duplicates(subset=["text"]).reset_index(drop=True)
while len(df)<MIN_TOTAL:
    df=pd.concat([df,pd.DataFrame(sample_rows(1))],ignore_index=True)\
         .drop_duplicates(subset=["text"]).reset_index(drop=True)
df=df.sample(frac=1.0,random_state=42).reset_index(drop=True)
df.to_csv(csv,index=False)
print("CSV listo:",csv,"shape:",df.shape)
PY
```

---

## ğŸš€ Comandos principales

### 1) Entrenamiento + evaluaciÃ³n (test)

```bash
python scripts/main_train.py --config configs/config_default.yaml
```

Salida:

* `reports/metrics_cls.json`
* `reports/figures/confusion_matrix.png`
* Artefactos en `models/artifacts/`

### 2) Inferencia rÃ¡pida (CLI)

```bash
python scripts/main_infer.py --config configs/config_default.yaml \
  --text "Paciente con dolor torÃ¡cico irradiado a brazo izquierdo" \
         "Molestia gÃ¡strica postprandial; indica dieta blanda."
```

### 3) AuditorÃ­a de sesgos por grupo (`sexo`)

```bash
python scripts/evaluate_bias.py --config configs/config_default.yaml
```

Salida:

* `reports/fairness/metrics_by_group.json`

> Si tu CSV tiene `template_id`, puedes activar un split por grupos (opcional) en el cÃ³digo para evitar que la misma plantilla caiga en train y test.


---

## ğŸ” Interpretabilidad de predicciones

El proyecto incluye un mÃ³dulo opcional de **explicabilidad** (`run_explainability.py`) que permite visualizar **quÃ© palabras del texto aportan mÃ¡s a la predicciÃ³n** del modelo.

### 1) Explicaciones con **LIME**

Genera archivos `.html` interactivos con los tÃ©rminos mÃ¡s relevantes para cada ejemplo de test.

```bash
python scripts/run_explainability.py \
  --config configs/config_default.yaml \
  --samples 5 \
  --method lime
```

* **QuÃ© hace:** toma 5 notas clÃ­nicas del conjunto de test, aplica LIME y guarda explicaciones en:

  ```
  reports/figures/explainability/lime_ex_0.html
  reports/figures/explainability/lime_ex_1.html
  ...
  ```
* **CÃ³mo usarlos:** abre los `.html` en el navegador para explorar visualmente quÃ© tokens influyen en la predicciÃ³n.

---

### 2) Explicaciones con **SHAP**

Genera grÃ¡ficos `.png` tipo barplot con los tokens mÃ¡s influyentes en la clasificaciÃ³n.

```bash
python scripts/run_explainability.py \
  --config configs/config_default.yaml \
  --samples 5 \
  --method shap
```

* **QuÃ© hace:** toma 5 notas clÃ­nicas de test, aplica SHAP y guarda figuras en:

  ```
  reports/figures/explainability/shap_ex_0.png
  reports/figures/explainability/shap_ex_1.png
  ...
  ```
* **CÃ³mo usarlos:** abre las imÃ¡genes para ver los tÃ©rminos mÃ¡s relevantes (positivos/negativos) para la clase predicha.

---

âš ï¸ **Nota:** Esta secciÃ³n es **opcional** en los requisitos.
El objetivo es mejorar la interpretabilidad del modelo y facilitar el anÃ¡lisis Ã©tico, pero no es obligatoria para la entrega mÃ­nima.


---

## ğŸ” Notas y buenas prÃ¡cticas

* **Sin fuga**: los textos NO contienen â€œleve/moderado/severoâ€.
* **EstratificaciÃ³n**: `stratify: true` pide que `test_size * n_samples â‰¥ n_clases`. Con datasets chicos, usa `test_size: 0.3`.
* **Vectorizador**: siempre `fit` en **train** y `transform` en **test**.
* **Resultados esperados**: la matriz de confusiÃ³n refleja **solo test** (no el total del CSV).

---

## ğŸ› ï¸ Troubleshooting

* `ValueError: Mix of label input types (string and number)`: asegÃºrate de que **label** sea **numÃ©rica** (0/1/2) tanto en `y_true` como en `y_pred`; el proyecto ya fuerza tipos consistentes en la auditorÃ­a.
* `The test_size = X should be â‰¥ #classes`: sube `test_size` o aumenta el dataset.
* `OSError: [E050] Can't find model 'es_core_news_sm'`: instala el modelo de spaCy (ver secciÃ³n Requisitos).

---




# ğŸ“Š Resultados y AnÃ¡lisis

## Resumen Ejecutivo

El sistema de clasificaciÃ³n de notas clÃ­nicas logrÃ³ procesar mÃ¡s de **100 registros simulados**, asignando cada texto a categorÃ­as de gravedad clÃ­nica: **leve, moderado y severo**.
Los resultados muestran un desempeÃ±o **consistente** con mÃ©tricas de precisiÃ³n macro superiores al 0.80, aunque aÃºn existen desafÃ­os en la **diferenciaciÃ³n entre clases limÃ­trofes** (moderado vs severo).

La auditorÃ­a de sesgos evidenciÃ³ que el modelo mantiene un rendimiento balanceado entre gÃ©neros (F/M), sin diferencias significativas en las mÃ©tricas de F1. Finalmente, los mÃ©todos de explicabilidad (LIME y SHAP) permitieron identificar las **palabras clave** que guÃ­an las predicciones, fortaleciendo la transparencia y confianza en el sistema.

---

## ğŸ“ˆ MÃ©tricas de ClasificaciÃ³n

Los principales indicadores obtenidos (ver `reports/metrics_cls.json`) fueron:

* **Accuracy global:** > 0.80
* **F1-macro:** estable en todas las ejecuciones
* **DistribuciÃ³n balanceada de errores** entre clases leves, moderadas y severas.

La matriz de confusiÃ³n (`reports/figures/confusion_matrix.png`) confirma que los casos leves y severos son mÃ¡s fÃ¡ciles de identificar, mientras que los moderados presentan mayor solapamiento.

---

## âš–ï¸ EvaluaciÃ³n de Sesgos

El anÃ¡lisis por grupos (`reports/fairness/metrics_by_group.json`) muestra:

* Rendimiento similar en **hombres y mujeres**.
* No se detectaron sesgos significativos asociados al atributo sensible `sexo`.
* La mÃ©trica **F1-macro** se mantuvo consistente al estratificar por gÃ©nero.

Esto sugiere que el sistema es razonablemente justo bajo el dataset utilizado.

---

## ğŸ” Interpretabilidad (LIME & SHAP)

Para mejorar la comprensiÃ³n del modelo:

* **LIME** (`reports/figures/explainability/lime_ex_*.html`) genera visualizaciones interactivas donde se destacan las palabras que aumentan o reducen la probabilidad de cada clase.
  Ejemplo: tÃ©rminos como *â€œcefaleaâ€, â€œleveâ€* tienden a impulsar la predicciÃ³n hacia **leve**, mientras que *â€œdisneaâ€, â€œedemaâ€* refuerzan la clasificaciÃ³n **severo**.

* **SHAP** (`reports/figures/explainability/shap_ex_*.png`) aporta una visiÃ³n agregada de la importancia de cada tÃ©rmino en el corpus, mostrando que palabras clÃ­nicas especÃ­ficas son determinantes para las decisiones del modelo.

Ambos enfoques permiten **auditar decisiones individuales** y **extraer patrones globales**, aportando transparencia en un contexto clÃ­nico sensible.

---

âœ… En conjunto, los resultados son **coherentes con lo esperado**: el sistema logra clasificar con buena precisiÃ³n, mantiene un comportamiento justo entre gÃ©neros y ofrece interpretabilidad de sus decisiones.

PrÃ³ximos pasos sugeridos:

* Ampliar el dataset con ejemplos reales o mÃ¡s variados.
* Explorar embeddings contextuales (ej. **FastText o BERT**) para mejorar la separaciÃ³n entre clases moderadas y severas.
* Integrar la reflexividad Ã©tica en despliegues reales (validaciÃ³n clÃ­nica, revisiÃ³n humana).



---

# ğŸ“¦ Artefactos (outputs) â€” VisualizaciÃ³n y verificaciÃ³n

Esta secciÃ³n te permite **ver rÃ¡pidamente** todas las salidas producidas por el proyecto y **reproducir su apertura** desde consola.

> Rutas relativas al root del repo.

## ğŸ§ª ClasificaciÃ³n â€” ImÃ¡genes (PNG)

**Matriz de confusiÃ³n**

![Confusion Matrix](reports/figures/confusion_matrix.png)

**SHAP â€” ejemplos explicados (top-k tÃ©rminos por documento)**

<img src="reports/figures/explainability/shap_ex_0.png" width="420">
<img src="reports/figures/explainability/shap_ex_1.png" width="420">
<img src="reports/figures/explainability/shap_ex_2.png" width="420">
<img src="reports/figures/explainability/shap_ex_3.png" width="420">
<img src="reports/figures/explainability/shap_ex_4.png" width="420">

> Si necesitas regenerarlos:
> `python scripts/run_explainability.py --config configs/config_default.yaml --method shap --samples 5`

---

## ğŸ§  Explicabilidad â€” LIME (HTML interactivo)

Abre en el navegador para explorar tokens que empujan la predicciÃ³n:

* [lime\_ex\_0.html](reports/figures/explainability/lime_ex_0.html)
* [lime\_ex\_1.html](reports/figures/explainability/lime_ex_1.html)
* [lime\_ex\_2.html](reports/figures/explainability/lime_ex_2.html)
* [lime\_ex\_3.html](reports/figures/explainability/lime_ex_3.html)
* [lime\_ex\_4.html](reports/figures/explainability/lime_ex_4.html)

**PNG (estÃ¡ticos):**
<p float="left">
  <img src="reports/figures/explainability/lime_ex_0.png" width="420" />
  <img src="reports/figures/explainability/lime_ex_1.png" width="420" />
</p>
<p float="left">
  <img src="reports/figures/explainability/lime_ex_2.png" width="420" />
  <img src="reports/figures/explainability/lime_ex_3.png" width="420" />
</p>
<p float="left">
  <img src="reports/figures/explainability/lime_ex_4.png" width="420" />
</p>

> Para regenerar:
> `python scripts/run_explainability.py --config configs/config_default.yaml --method lime --samples 5`

---

## ğŸ“‘ MÃ©tricas â€” JSON

* MÃ©tricas de clasificaciÃ³n: [`reports/metrics_cls.json`](reports/metrics_cls.json)
* Fairness por grupo (p. ej. `sexo`): [`reports/fairness/metrics_by_group.json`](reports/fairness/metrics_by_group.json)







> Todos los artefactos se guardan bajo `reports/` (y `reports/figures/explainability/` para LIME/SHAP). Si ejecutas desde otro directorio, asegÃºrate de correr los comandos **desde la raÃ­z del repo**.

---

## ğŸ“ InterpretaciÃ³n breve de los resultados

* La matriz de confusiÃ³n evidencia buena separaciÃ³n para **leve** y **severo**; **moderado** muestra algo de solapamiento (esperable por la cercanÃ­a semÃ¡ntica).
* Las mÃ©tricas en `metrics_cls.json` apuntan a un rendimiento **sÃ³lido y consistente** (macro-F1 alto).
* En `fairness/metrics_by_group.json` no se observan diferencias relevantes entre grupos por `sexo`, lo que sugiere un comportamiento **balanceado** del clasificador en el dataset actual.
* Las figuras **SHAP** y los **HTML de LIME** explican quÃ© tÃ©rminos impulsan cada predicciÃ³n, confirmando que el modelo se apoya en seÃ±ales clÃ­nicas plausibles (*â€œdolor torÃ¡cicoâ€, â€œdisneaâ€, â€œedemaâ€*, etc.).

> RecomendaciÃ³n: ampliar el dataset (cantidad y variedad), y probar embeddings (Word2Vec/FastText/transformers) para mejorar la separaciÃ³n entre **moderado** y **severo**.

---




# ğŸ“Š Resultados y comparaciÃ³n de modelos

## ğŸ§­ Resumen ejecutivo

Entrenamos y comparamos varios clasificadores de gravedad clÃ­nica (0=leve, 1=moderado, 2=severo) usando diferentes representaciones de texto:

* **Multinomial Naive Bayes (MNB) + TF-IDF**
* **Linear SVM + Word2Vec**
* **Logistic Regression + FastText**
* **Linear SVM + Embeddings tipo BERT** (Sentence-Transformers; â€œTransformer preentrenadoâ€)

En el conjunto de prueba, el mejor balance entre precisiÃ³n y recall macro lo obtuvo **Linear SVM + Embeddings Transformer**, con **accuracy â‰ˆ 0.90** y **F1-macro â‰ˆ 0.904**. A nivel de clases, el desempeÃ±o fue especialmente alto en la clase **2 (severo)**, con precisiÃ³n y recall cercanos a 1.0.&#x20;

---

## âš™ï¸ CÃ³mo reproducir los 4 experimentos

> Cada corrida guarda sus artefactos en una carpeta Ãºnica con sello de tiempo:
> `reports/runs/<YYYY-MM-DD_HH-MM-SS>_<modelo>_<representacion>_<tag>/`

```bash
# 1) Multinomial Naive Bayes + TF-IDF
python scripts/main_train.py --config configs/config_default.yaml \
  --model multinomial_nb --rep tfidf --tag mnb_tfidf

# 2) Linear SVM + Word2Vec (entrenado en tus datos)
python scripts/main_train.py --config configs/config_default.yaml \
  --model linear_svm --rep word2vec --tag svm_w2v

# 3) Logistic Regression + FastText (entrenado en tus datos)
python scripts/main_train.py --config configs/config_default.yaml \
  --model logreg --rep fasttext --tag logreg_ft

# 4) Linear SVM + Embeddings Transformer (BETO/DistilUSE m-multilingual por defecto)
python scripts/main_train.py --config configs/config_default.yaml \
  --model linear_svm --rep transformer_embed --tag svm_st
```

> Notas:
>
> * MNB solo es compatible con representaciones **no negativas** (TF-IDF/BoW).
> * Los outputs (modelo, vectorizador/embeddings, mÃ©tricas, figuras) quedan en `reports/runs/...`.
> * El flujo â€œcarga â†’ preprocesa â†’ vectoriza/embeddings â†’ entrena â†’ evalÃºa â†’ guarda artefactosâ€ estÃ¡ implementado en el pipeline de entrenamiento.&#x20;

---

## ğŸ§ª Â¿QuÃ© hace cada modelo?

* **Multinomial Naive Bayes (MNB)**: modelo probabilÃ­stico muy eficiente en textos con **TF-IDF/BoW**; supone independencia condicional entre tÃ©rminos. Ideal como baseline rÃ¡pido.
* **Linear SVM**: maximiza el margen entre clases en un espacio de alta dimensiÃ³n; funciona muy bien con representaciones densas (p. ej., **Word2Vec** o **Transformers**).
* **Logistic Regression**: lineal y probabilÃ­stico; buen baseline con embeddings (**FastText**).
* **Embeddings Transformer (Sentence-Transformers)**: vectoriza cada texto con un **modelo preentrenado tipo BERT** (p. ej., `distiluse-base-multilingual-cased-v2` o **BETO** si lo configuras), capturando contexto semÃ¡ntico; luego un clasificador lineal aprende sobre esos vectores.

---

## ğŸ–¼ï¸ VisualizaciÃ³n â€” Matrices de confusiÃ³n

> Cada corrida guarda su figura en:
> `reports/runs/<RUN_DIR>/figures/confusion_matrix.png`

Ejemplos (si ya ejecutaste los 4, sustituye `<RUN_DIR_*>` por las carpetas reales que te imprimiÃ³ el script):

### Transformer (Linear SVM + Transformer embeddings)

**MÃ©tricas (ejemplo real)**: accuracy â‰ˆ **0.90**, F1-macro â‰ˆ **0.904**.&#x20;

```
reports/runs/2025-09-06_00-20-24_linear_svm_transformer_embed_svm_st/figures/confusion_matrix.png
```

![Confusion â€” linear\_svm | transformer\_embed](reports/runs/2025-09-06_00-20-24_linear_svm_transformer_embed_svm_st/figures/confusion_matrix.png)

### Multinomial NB + TF-IDF

```
reports/runs/<RUN_DIR_MNB_TFIDF>/figures/confusion_matrix.png
```

![Confusion â€” multinomial\_nb | tfidf](reports/runs/2025-09-06_00-19-13_multinomial_nb_tfidf_mnb_tfidf/figures/confusion_matrix.png)

### Linear SVM + Word2Vec

```
reports/runs/<RUN_DIR_SVM_W2V>/figures/confusion_matrix.png
```

![Confusion â€” linear\_svm | word2vec](reports/runs/2025-09-06_00-19-17_linear_svm_word2vec_svm_w2v/figures/confusion_matrix.png)

### Logistic Regression + FastText

```
reports/runs/<RUN_DIR_LOGREG_FT>/figures/confusion_matrix.png
```

![Confusion â€” logreg | fasttext](reports/runs/2025-09-06_00-19-22_logreg_fasttext_logreg_ft/figures/confusion_matrix.png)

---

## ğŸ“ˆ MÃ©tricas principales (ejemplo Transformer)

> ExtraÃ­das del JSON de la corrida **Linear SVM + Transformer embeddings**.&#x20;

* **Accuracy**: `0.90`
* **F1-macro**: `0.9039`
* **Por clase (0/1/2)** â€“ *precision / recall / F1*:

  * **0 (leve)**: 0.769 / 1.000 / 0.870
  * **1 (moderado)**: 1.000 / 0.727 / 0.842
  * **2 (severo)**: 1.000 / 1.000 / 1.000

> Ruta del JSON:
> `reports/runs/2025-09-06_00-20-24_linear_svm_transformer_embed_svm_st/metrics_cls.json`

---



## âœ… Checklist de requisitos del enunciado

* **Al menos 2 modelos**: âœ“ MNB+TF-IDF y SVM+Transformer (ademÃ¡s: SVM+W2V y LogReg+FastText).
* **Transformer preentrenado**: âœ“ Sentence-Transformers (puedes configurar **BETO** en `features.transformer_embed.model`).
* **ComparaciÃ³n de mÃ©tricas por clase**: âœ“ JSON + matrices de confusiÃ³n.
* **Preprocesamiento clÃ¡sico** (minÃºsculas, URLs/emails, tokenizaciÃ³n, lemas/stopwords): âœ“ ya integrado en el pipeline.&#x20;
* **Representaciones**: âœ“ TF-IDF, Word2Vec, FastText y Embeddings Transformer.
* **EvaluaciÃ³n estratificada por atributo sensible** (sexo): âœ“ mediante scripts de fairness (si activas `sensitive_attr`/`sensitive_col` en `config_default.yaml`).

---

### Sugerencias para el informe

* Incluye una tabla comparando **Accuracy** y **F1-macro** de cada modelo.
* Comenta los trade-offs: TF-IDF+MNB (baseline rÃ¡pido) vs embeddings densos (Transformers) con mejor captura semÃ¡ntica.
* AÃ±ade capturas de SHAP/LIME para ejemplos representativos (opcional pero muy valorado).

---
