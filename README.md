# üìä Parte A ‚Äì LSTM para Clasificaci√≥n de Sentimiento (IMDb)

## üéØ Introducci√≥n
En esta actividad implementamos una red neuronal recurrente (LSTM) para clasificar sentimientos en rese√±as de pel√≠culas utilizando el dataset **IMDb**.  
El objetivo fue **comprender el comportamiento de las secuencias en un modelo recurrente**, observar sus limitaciones, y reflexionar sobre los desaf√≠os que enfrenta para ‚Äúrecordar‚Äù informaci√≥n contextual a lo largo de la secuencia.

---

## üìå Resumen del experimento
- **Dataset**: IMDb (25k train, 25k test; binario positivo/negativo).
- **Modelo**: Embedding (128d) + LSTM (64 unidades, dropout/recurrent_dropout + regularizaci√≥n L2) + Dense(sigmoid).
- **Entrenamiento**:  
  - √âpocas: 10  
  - Batch size: 64  
  - Optimizador: Adam (lr=2e-4, clipnorm=1.0)  
  - Validaci√≥n: 20% split  
- **Regularizaci√≥n aplicada**: Dropout (0.5), recurrent_dropout (0.3), L2 (1e-4).
- **Callbacks**: EarlyStopping, ReduceLROnPlateau, ModelCheckpoint.

---

## üìà Resultados de entrenamiento

![Curvas de accuracy/loss](outputs_imdb/imdb_lstm__adam2e-4_clip1.0__bce__vs0.2__bs64__ep10_curves.png)

### M√©tricas finales
- **Accuracy final (train)**: 0.9722  
- **Accuracy final (val)**: 0.8830  
- **Mejor val_accuracy**: 0.8882 (√©poca 5)  
- **Gap train‚Äìval**: 0.0892 (indicador de sobreajuste moderado)  
- **Oscilaciones en val_loss**: 5 subidas detectadas  
- **Test accuracy**: 0.8638  

üìä **Matriz de confusi√≥n (test set)**  
![Confusion matrix](outputs_imdb/imdb_lstm__adam2e-4_clip1.0__bce__vs0.2__bs64__ep10_confusion_matrix.csv)

---

## üßê Reflexi√≥n sobre los resultados
- La red logra un desempe√±o s√≥lido (**test ‚âà 86%**), dentro del rango esperado para un LSTM sencillo en IMDb.  
- Existe un **gap de ~0.09** entre entrenamiento y validaci√≥n, lo que indica cierto **sobreajuste**. Esto es com√∫n en secuencias largas, ya que la LSTM tiende a memorizar patrones frecuentes del set de entrenamiento.  
- La **val_loss muestra oscilaciones** despu√©s de la √©poca 3, lo que sugiere sensibilidad al orden de batches y la complejidad del lenguaje natural.  
- La dificultad principal est√° en **‚Äúrecordar‚Äù dependencias largas** en las rese√±as (palabras relevantes al inicio vs. conclusi√≥n). Esto explica por qu√© la red logra generalizar, pero no alcanza rangos superiores (>90%).  
- Con m√°s recursos podr√≠amos:
  - Usar embeddings preentrenados (GloVe, Word2Vec).
  - Incrementar `maxlen` para capturar m√°s contexto.
  - Probar arquitecturas m√°s robustas (BiLSTM, GRU o Transformers).

---

## ‚úÖ Conclusi√≥n
El modelo LSTM implementado cumple los objetivos de la actividad:
- Aprende representaciones secuenciales de las rese√±as.
- Generaliza en el rango esperado (~86% en test).
- Evidencia limitaciones para manejar memorias largas y prevenir sobreajuste.  

Esto aporta evidencia de la **dificultad de ‚Äúrecordar‚Äù secuencias completas**, lo cual justifica la evoluci√≥n hacia arquitecturas m√°s avanzadas (como Transformers) en procesamiento de lenguaje natural.
